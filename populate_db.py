import requests
import time
from bs4 import BeautifulSoup
from models import SessionLocal, Source, Article
import os

# [ìˆ˜ì •] AWS ì„œë²„ ë‚´ë¶€(Localhost)ì—ì„œ ì‹¤í–‰ë˜ëŠ” API ã…‡ã…‡ã…ã„´
API_URL = "http://127.0.0.1:8000/search"

# [í•µì‹¬] ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •
CATEGORY_KEYWORDS = {
    "politics": ["ì •ì¹˜", "ëŒ€í†µë ¹", "êµ­íšŒ", "ì—¬ë‹¹", "ì•¼ë‹¹", "ì´ì„ "],
    "economy": ["ê²½ì œ", "ì£¼ì‹", "ì‚¼ì„±ì „ì", "ë¶€ë™ì‚°", "ê¸ˆë¦¬", "í™˜ìœ¨"],
    "society": ["ì‚¬íšŒ", "ì‚¬ê±´", "ë‚ ì”¨", "êµí†µ", "í™˜ê²½"],
    "world": ["êµ­ì œ", "ë¯¸êµ­", "ì¤‘êµ­", "ì „ìŸ"],
    "tech": ["IT", "ì¸ê³µì§€ëŠ¥", "AI", "ìŠ¤ë§ˆíŠ¸í°", "ê³¼í•™"],
    "entertainment": ["ì—°ì˜ˆ", "ì˜í™”", "ë“œë¼ë§ˆ", "ì•„ì´ëŒ", "ë°°ìš°"],
    "sports": ["ìŠ¤í¬ì¸ ", "ì¶•êµ¬", "ì•¼êµ¬", "ì†í¥ë¯¼", "ì˜¬ë¦¼í”½"]
}

def get_details_from_html(url):
    """
    ê¸°ì‚¬ URLë¡œ ì ‘ì†í•´ ì¹´í…Œê³ ë¦¬, ê¸°ì ì´ë¦„, ê³ í™”ì§ˆ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=3)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. ê¸°ì ì´ë¦„ ì¶”ì¶œ
        reporter_name = None
        reporter_tag = soup.select_one('.media_end_head_journalist_name') or \
                       soup.select_one('.byline_s') or \
                       soup.select_one('.journalist_name')
        if reporter_tag:
            reporter_name = reporter_tag.get_text(strip=True).split(' ')[0]

        # 2. ê³ í™”ì§ˆ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        image_url = None
        og_image = soup.select_one('meta[property="og:image"]')
        if og_image: 
            image_url = og_image['content']
        
        return reporter_name, image_url
    except Exception:
        return None, None

def run_populate():
    db = SessionLocal()
    total_saved = 0
    
    print(">>> ğŸ“¡ [AWS] ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìƒì„¸ ì •ë³´ ë³´ê°• ì‹œì‘...")

    for category, keywords in CATEGORY_KEYWORDS.items():
        print(f"  ğŸ“‚ [{category}] ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘ (í‚¤ì›Œë“œ: {keywords})...")
        
        for keyword in keywords:
            try:
                # 1. API ê²€ìƒ‰ ìš”ì²­ (ë¡œì»¬í˜¸ìŠ¤íŠ¸ë¡œ ì „ì†¡)
                response = requests.get(API_URL, params={"query": keyword})
                if response.status_code != 200:
                    print(f"    ! API ì˜¤ë¥˜ ({keyword}): ìƒíƒœì½”ë“œ {response.status_code}")
                    continue
                    
                data = response.json()
                items = data.get("items", [])
                
                saved_count_in_keyword = 0
                for item in items:
                    link = item['link']
                    
                    # ì¤‘ë³µ í™•ì¸
                    exists = db.query(Article).filter(Article.url == link).first()
                    if exists: 
                        continue
                    
                    # ìƒì„¸ ì •ë³´ ê¸ì–´ì˜¤ê¸°
                    real_reporter_name, hq_image_url = get_details_from_html(link)
                    
                    # HTML íƒœê·¸ ì •ë¦¬
                    title = item['title'].replace("<b>", "").replace("</b>", "").replace("&quot;", "'")
                    description = item['description'].replace("<b>", "").replace("</b>", "")
                    
                    # ì–¸ë¡ ì‚¬ í™•ì¸ ë° ìƒì„±
                    source = db.query(Source).filter(Source.name == "ë„¤ì´ë²„ë‰´ìŠ¤").first()
                    if not source:
                        source = Source(name="ë„¤ì´ë²„ë‰´ìŠ¤", bias_label="unknown")
                        db.add(source)
                        db.commit()
                        db.refresh(source)
                    
                    # ì €ì¥
                    article = Article(
                        title=title, 
                        url=link, 
                        body=description, 
                        source_id=source.id, 
                        topic_id=None,
                        category=category,
                        reporter_name=real_reporter_name,
                        image_url=hq_image_url if hq_image_url else None
                    )
                    db.add(article)
                    saved_count_in_keyword += 1
                    
                    time.sleep(0.1) # ì°¨ë‹¨ ë°©ì§€
                
                db.commit()
                if saved_count_in_keyword > 0:
                    print(f"    - '{keyword}': {saved_count_in_keyword}ê°œ ì €ì¥ë¨")
                total_saved += saved_count_in_keyword
                
            except Exception as e:
                print(f"    ! ì˜¤ë¥˜ ë°œìƒ ({keyword}): {e}")
                pass
            
    db.close()
    print(f"\nğŸ‰ ì´ {total_saved}ê°œì˜ ê¸°ì‚¬ê°€ ìƒì„¸ ì •ë³´ì™€ í•¨ê»˜ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    run_populate()
